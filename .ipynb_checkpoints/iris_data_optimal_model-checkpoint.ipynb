{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a38910",
   "metadata": {},
   "source": [
    "[Link to GitHub Repo](http://localhost:8888/notebooks/Desktop/iris_for_hyperparameter_tuning-master/iris_data_optimal_model.ipynb)\n",
    "### Hyperparameter Tuning\n",
    "**Introduction to the project**<br>\n",
    "The process of choosing optimal parameter is called *Hyperparameter Tuning*. Here in this project, I have tried to show the following for a simple and popular iris flower data directly imported from `sklearn` library.\n",
    "\n",
    "I have divided the project mainly into following four areas. \n",
    "- Explanatory Data Analysis\n",
    "- Preprocessing of the data\n",
    "- Hyperparameter Tuning\n",
    "- Selection of the best model\n",
    "\n",
    "I have tried to find out a model from different models that would get the optimal prediction.\n",
    "Following are the models that are being used here for fine-tuning.\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e194b84",
   "metadata": {},
   "source": [
    "#### Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ddb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the source library\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(dir(iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502089da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can uncomment the following code to read about the dataset.\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# know the classes of iris in the data, (or target)\n",
    "f'target_names: {iris.target_names}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'feature_names: {iris.feature_names}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'filename: {iris.filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dec87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'data: {iris.data[:5]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form dataframe by using pandas library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append a column 'target'\n",
    "df['target'] = iris.target\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43725c8f",
   "metadata": {},
   "source": [
    "Below listed are the attribute information for the dataset.\n",
    "- `sepal length (cm)`\n",
    "- `sepal width (cm)`\n",
    "- `petal length (cm)`\n",
    "- `petal width (cm)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f1486",
   "metadata": {},
   "source": [
    "Let's now convert the `target` column into iris_class by their respective classified names. Python lambda function has been used for the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7701af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append one more column 'iris class'\n",
    "df['iris class'] = iris.target\n",
    "df['iris class'] = df['iris class'].apply(lambda x: iris.target_names[x])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5724cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info on data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb40c23",
   "metadata": {},
   "source": [
    "Numerical features: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)<br>\n",
    "Categorical feature: iris class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646854f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for each label\n",
    "df['iris class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fdf67",
   "metadata": {},
   "source": [
    "**Preprocessing the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f347d",
   "metadata": {},
   "source": [
    "The dataset has 150 samples in total with three different categories, 50 instances each. We now check the null values, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf169cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all missing values, if present\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bc52a",
   "metadata": {},
   "source": [
    "The above method drops the samples that contains a null value. By default, it drops rows having any null value. It can be made to drop columns too. Similarly, we can consider a row or column to drop if all vlaues are null. By default, it's `any`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc448f7e",
   "metadata": {},
   "source": [
    "**Data visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# visualization as histogram for each dimension of dataset\n",
    "# label column dropped\n",
    "for col in df.columns.drop('iris class'):\n",
    "    plt.title(col[:-4].title()+'Histogram')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('values')\n",
    "    df[col].hist()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d738af8",
   "metadata": {},
   "source": [
    "**Scatterplot**<br>\n",
    "Python technique for plotting `pyplot` has been used for visualization of different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = dict(b=['setosa', '+'], g=['versicolor', '.'], r=['virginica', '*']) \n",
    "\n",
    "#assuming (b,g,r) as (blue,green,red)\n",
    "# dictionary, each key with pair of axes as value \n",
    "\n",
    "dims = dict(\n",
    "    one = ['sepal length (cm)', 'sepal width (cm)'],\n",
    "    two = ['sepal length (cm)', 'petal length (cm)'],\n",
    "    three = ['sepal width (cm)', 'petal width (cm)'],\n",
    "    four = ['petal length (cm)', 'petal width (cm)']\n",
    ")\n",
    "# iterations to plot for each pair\n",
    "for k in dims:\n",
    "    for cl in cls:\n",
    "        x = df[df['iris class']==cls[cl][0]]\n",
    "        plt.scatter(x[dims[k][0]], x[dims[k][1]], c=cl, label=cls[cl][0], marker=cls[cl][1])\n",
    "    plt.xlabel(dims[k][0])\n",
    "    plt.ylabel(dims[k][1])\n",
    "    plt.title(f'Scatter-plot: {dims[k][0][:-4]} vs {dims[k][1][:-4]}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c5376",
   "metadata": {},
   "source": [
    "**Correlation matrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149f0ae",
   "metadata": {},
   "source": [
    "A correlation matrix is a table showing correlation coefficients between two variables. Each cell in the table shows the correlation between two variables. The value is in between -1 and 1. If two variables have high correlation, we can neglect one variable from those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation table\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62382dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# store the corr-matrix\n",
    "corr = df.corr()\n",
    "# representation or correlation table as heatmap\n",
    "# coefficients ranging from low to high, represented by color depth.\n",
    "# use of seaborn library\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(corr, annot=True, ax=ax, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7db0a",
   "metadata": {},
   "source": [
    "**Data split**<br>\n",
    "For all our computational work, we are going to adapt the data available in the dataset `iris` instead of `df` because it does not contain any unwanted feature.\n",
    "\n",
    "Now, let's split the dataset by using `train_test_split` method from `sklearn.model_selection` with test to train ratio 3:7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(iris.data, iris.target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56c3fb",
   "metadata": {},
   "source": [
    "Now, let's use the `svm` model to train the model and calculate score. I will randomly initialize the parameters because it's not known which parameters yield the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC(kernel='rbf', C=3, gamma='auto') \n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_model.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9f71f",
   "metadata": {},
   "source": [
    "Above model yields different scores each time based upon samples. For the same reason, we use K-fold cross validation. In K-fold cross validation, we assign `n` number of folds in the dataset and it takes `n` iterations. Out of `n` folds, `1/n` portion is test set and `(n-1)/n` is `train set`. `Train and test sets` will get shifting in each iteration so that either set does repeat. In other words, each iteration will operate on a new dataset to get scores individually and derive an average.\n",
    "\n",
    "For different values of `kernel`, we can run a `python loop`. For each value of `kernel`, we can assign one or a number of values for the folds to be used in cross validating so that we can obtain a score which is more realistic and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter tuning\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kernels = ['rbf', 'linear']\n",
    "C = [1, 10, 20]\n",
    "avg_scores = dict()\n",
    "for kval in kernels:\n",
    "    for cval in C:\n",
    "        cv_scores = cross_val_score(svm.SVC(kernel=kval, C=cval, \n",
    "                                    gamma='auto'), iris.data, iris.target, cv=5)\n",
    "        avg_scores[kval+'_'+str(cval)]=np.average(cv_scores)\n",
    "avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8366143",
   "metadata": {},
   "source": [
    "Thus, we can see that diff sets of parameters yield different scores in each iteration. However, this method is not a practical approach and not convenient. It's very costly for a data scientist and processing will be even costlier when the size of dataset is big enough.\n",
    "\n",
    "`Scikit-learn` has a model called `GridSearchCV` which does exactly the same thing as above but in a very convenient way.\n",
    "\n",
    "Once the model is done, we will fit the model in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(svm.SVC(gamma='auto'), \n",
    "                   dict(C=[1, 10, 20], kernel=['rbf', 'linear']),\n",
    "                   cv=5, return_train_score=False)\n",
    "clf.fit(iris.data, iris.target)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f2363",
   "metadata": {},
   "source": [
    "It's not all that easy to read this output. But we can read this easily by converting it into pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results = pd.DataFrame(clf.cv_results_)\n",
    "score_results.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9265e",
   "metadata": {},
   "source": [
    "Let us now trim this dataframe down to have only the columns that are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results[['param_C', 'param_kernel', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfde1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(\n",
    "    svm.SVC(gamma='auto'), \n",
    "    dict(C=[1,10,20], kernel=['rbf','linear']), \n",
    "    cv=5, \n",
    "    return_train_score=False, \n",
    "    n_iter=2\n",
    ")\n",
    "\n",
    "rs.fit(iris.data, iris.target)\n",
    "pd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ae5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params': {\n",
    "            'C': [1,10,20],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }\n",
    "    },\n",
    "    'random_forest':{\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model_name, mp in model_params.items():\n",
    "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(iris.data, iris.target)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be005051",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores, columns=['model','best_score','best_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d829193",
   "metadata": {},
   "source": [
    "**Conclusion**<br>\n",
    "Having all computation done, it is found that `svm` model yields the best result with optimal parameters slected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "    table.dataframe td, table.dataframe th {\n",
    "        border: .3px black solid !important;\n",
    "        color: black !important;\n",
    "    }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
